{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# 检查是否已有 SparkSession，如果没有则创建\n",
    "if 'spark' not in globals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"flightProblem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# 加载航班数据（假设数据存储在 HDFS 或本地文件系统）\n",
    "data = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs://localhost:9000/airport\")\n",
    "\n",
    "# 选取问题所需特征\n",
    "dropList = ['FlightNum', 'ArrDelay']\n",
    "# 选出所需特征列\n",
    "stats = data.select([column for column in data.columns if column in dropList])\n",
    "# 清洗数据\n",
    "stats = stats[~stats['ArrDelay'].isin(['NA'])]\n",
    "# 展示结果\n",
    "stats.orderBy(-stats['ArrDelay']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "if 'spark' not in globals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"flightProblem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs://localhost:9000/airport\")\n",
    "\n",
    "\n",
    "# 构建延误率计算的数据集\n",
    "def rateOfDelay():\n",
    "    # 定义需要保留的列\n",
    "    dropList = ['UniqueCarrier', 'ArrDelay']\n",
    "\n",
    "    # 选取保留列作为初始数据集\n",
    "    stats = data.select(\n",
    "        [column for column in data.columns if column in dropList])\n",
    "\n",
    "    # 过滤掉 `ArrDelay` 列中值为 'NA' 的行\n",
    "    stats = stats[~stats['ArrDelay'].isin(['NA'])]\n",
    "\n",
    "    # 筛选延误时间大于 0 的航班，并按航空公司分组统计数量\n",
    "    statsDelay = stats.where(stats['ArrDelay'] > 0).groupBy(\n",
    "        'UniqueCarrier').count()\n",
    "\n",
    "    # 筛选延误时间小于等于 0 的航班，并按航空公司分组统计数量\n",
    "    statsNodelay = stats.where(stats['ArrDelay'] <= 0).groupBy(\n",
    "        'UniqueCarrier').count()\n",
    "\n",
    "    # 重命名未延误航班统计列\n",
    "    statsNodelay = statsNodelay.withColumnRenamed('count', 'NoDelayTimes')\n",
    "\n",
    "    # 重命名延误航班统计列\n",
    "    statsDelay = statsDelay.withColumnRenamed('count', 'DelayTimes')\n",
    "\n",
    "    # 合并延误航班和未延误航班的统计结果\n",
    "    statsMerge = statsDelay.join(\n",
    "        statsNodelay, on='UniqueCarrier', how='left_outer')\n",
    "\n",
    "    # 计算延误率，避免分母为 0 的情况\n",
    "    statsMerge = statsMerge.withColumn(\n",
    "        'DelayRate',\n",
    "        F.when(statsMerge['NoDelayTimes'] != 0, statsMerge['DelayTimes'] /\n",
    "               statsMerge['NoDelayTimes']).otherwise(1)\n",
    "    )\n",
    "\n",
    "    # 展示结果\n",
    "    statsMerge.show()\n",
    "\n",
    "    # 返回合并后的数据集\n",
    "    return statsMerge\n",
    "\n",
    "\n",
    "# 绘制延误率柱状图\n",
    "def reverseCapture():\n",
    "    # 获取延误率统计数据并转换为 pandas DataFrame\n",
    "    statsMerge = rateOfDelay()\n",
    "    pandasDelay = statsMerge.toPandas()\n",
    "\n",
    "    # 绘制水平柱状图\n",
    "    pandasDelay.DelayRate.plot(kind='barh')\n",
    "\n",
    "    # 设置 x 轴标签为延误率\n",
    "    plt.xlabel('DelayRate')\n",
    "\n",
    "    # 设置 y 轴标签为航空公司\n",
    "    plt.ylabel('UniqueCarrier')\n",
    "\n",
    "    # 设置 y 轴刻度为航空公司名称\n",
    "    a = np.arange(len(pandasDelay.UniqueCarrier))\n",
    "    plt.yticks(a, pandasDelay.UniqueCarrier)\n",
    "\n",
    "    # 设置 x 轴刻度旋转角度\n",
    "    plt.xticks(rotation=15)\n",
    "\n",
    "    # 设置标题\n",
    "    plt.title(\"DelayRate\")\n",
    "\n",
    "    # 显示图表\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "reverseCapture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "import matplotlib.dates as mdate\n",
    "\n",
    "\n",
    "if 'spark' not in globals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"flightProblem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs://localhost:9000/airport\")\n",
    "\n",
    "# 定义需要保留的列\n",
    "dropList = ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'ArrDelay']\n",
    "\n",
    "# 选择需要的特征列作为初始数据集\n",
    "stats = data.select([column for column in data.columns if column in dropList])\n",
    "\n",
    "# 过滤掉 `ArrDelay` 列中值为 'NA' 的行\n",
    "stats = stats[~stats['ArrDelay'].isin(['NA'])]\n",
    "\n",
    "# 按天聚合航班数据，计算每天的最大延误时间并加入新列 `maxDelayInOneDay`\n",
    "statsDay = stats.groupBy('Year', 'Month', 'DayofMonth', 'DayOfWeek') \\\n",
    "    .agg(F.max('ArrDelay').alias('maxDelayInOneDay'))\n",
    "\n",
    "# 对聚合后的数据按日期升序排列\n",
    "statsDay = statsDay.orderBy('Year', 'Month', 'DayofMonth')\n",
    "\n",
    "# 展示前 50 行结果\n",
    "statsDay.show(50)\n",
    "\n",
    "# 将聚合结果转换为 pandas DataFrame\n",
    "pandasDelay = statsDay.toPandas()\n",
    "\n",
    "# 绘制最大延误时间的折线图\n",
    "pandasDelay.maxDelayInOneDay.plot()\n",
    "\n",
    "# 设置 x 轴日期格式为 '2008/月/日'\n",
    "plt.gca().xaxis.set_major_formatter(mdate.DateFormatter('2008/%m/%d'))\n",
    "\n",
    "# 自动调整日期标签角度以防止重叠\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "# 设置 x 轴标签为日期\n",
    "plt.xlabel('date')\n",
    "\n",
    "# 设置 y 轴标签为到达延误时间\n",
    "plt.ylabel('ArrDelay')\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import concat_ws\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if 'spark' not in globals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"flightProblem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs://localhost:9000/airport\")\n",
    "\n",
    "# 定义要保留的列名\n",
    "dropList = ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'ArrDelay']\n",
    "\n",
    "# 从数据集中选取保留的特征列\n",
    "stats = data.select([column for column in data.columns if column in dropList])\n",
    "\n",
    "# 清洗数据，过滤掉 `ArrDelay` 列中值为 'NA' 的行\n",
    "stats = stats[~stats['ArrDelay'].isin(['NA'])]\n",
    "\n",
    "# 按日期分组，计算每天的最大到达延误时间\n",
    "statsDay = stats.groupBy('Year', 'Month', 'DayofMonth', 'DayOfWeek').agg(\n",
    "    F.max('ArrDelay').alias('maxDelayInOneDay')\n",
    ")\n",
    "\n",
    "# 对结果按日期排序\n",
    "statsDay = statsDay.orderBy('Year', 'Month', 'DayofMonth')\n",
    "\n",
    "# 创建新的列 `date`，将年、月、日连接成日期格式\n",
    "df = statsDay.withColumn('date', concat_ws(\n",
    "    '-', statsDay['Year'], statsDay['Month'], statsDay['DayofMonth']))\n",
    "\n",
    "# 设置初始年份为 2008\n",
    "initial_date = 2008\n",
    "\n",
    "# 选择需要的列进行计算\n",
    "df = df.select('maxDelayInOneDay', 'date')\n",
    "\n",
    "# 计算年数、月数、天数和周数\n",
    "df2 = df.withColumn(\n",
    "    'yearCal',\n",
    "    F.year('date') - initial_date\n",
    ").withColumn(\n",
    "    'monthCal',\n",
    "    F.month('date') + F.col('yearCal') * 12\n",
    ").withColumn(\n",
    "    'dayCal',\n",
    "    F.datediff('date', F.lit('%s-01-01' % initial_date)) + 1\n",
    ").withColumn(\n",
    "    'weekNum',\n",
    "    (F.col('dayCal') / 7).cast('int')\n",
    ")\n",
    "\n",
    "# 选择所需列并按周数分组，统计每周的最大延误时间\n",
    "statsWeek = df2.select('maxDelayInOneDay', 'date', 'weekNum')\n",
    "statsWeek = statsWeek.groupBy('weekNum').agg(\n",
    "    F.max('maxDelayInOneDay').alias('maxDelayInOneWeek')\n",
    ")\n",
    "\n",
    "# 展示前 50 行结果\n",
    "statsWeek.orderBy('weekNum').show(50)\n",
    "\n",
    "# 转换为 pandas DataFrame 以便绘制柱状图\n",
    "pandasDelay = statsWeek.toPandas()\n",
    "\n",
    "# 绘制延误时间的水平柱状图\n",
    "pandasDelay.maxDelayInOneWeek.plot(kind='barh')\n",
    "\n",
    "# 设置 x 轴标签为到达延误时间\n",
    "plt.xlabel('ArrDelay')\n",
    "\n",
    "# 设置 y 轴标签为周数\n",
    "plt.ylabel('Week')\n",
    "\n",
    "# 设置 y 轴刻度字体大小\n",
    "plt.yticks(fontsize=7)\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import bround\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if 'spark' not in globals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"flightProblem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs://localhost:9000/airport\")\n",
    "\n",
    "# 将 Cancelled 列显式转换为整数类型，以避免类型问题\n",
    "data = data.withColumn('Cancelled', data['Cancelled'].cast('int'))\n",
    "\n",
    "\n",
    "def badCancelRate():\n",
    "    # 选取所需列，并添加 FlightType 列用于航班分类\n",
    "    # 0 表示短程航班，1 表示长程航班。\n",
    "    stats = data.select('Distance', 'Cancelled')\n",
    "    stats = stats.withColumn('FlightType', F.when(\n",
    "        stats['Distance'] <= 500, 0).otherwise(1))\n",
    "\n",
    "    # 按 FlightType 统计取消航班数目\n",
    "    statsCancel = stats.where(\n",
    "        stats['Cancelled'] == 1).groupBy('FlightType').count()\n",
    "    statsCancel = statsCancel.withColumnRenamed('count', 'CancelNum')\n",
    "\n",
    "    # 按 FlightType 统计未取消航班数目\n",
    "    statsNocancel = stats.where(\n",
    "        stats['Cancelled'] == 0).groupBy('FlightType').count()\n",
    "    statsNocancel = statsNocancel.withColumnRenamed('count', 'NoCancelNum')\n",
    "\n",
    "    # 合并取消和未取消统计数据\n",
    "    statsMerge = statsCancel.join(\n",
    "        statsNocancel, on='FlightType', how='left_outer')\n",
    "\n",
    "    # 计算总航班数和取消率\n",
    "    statsMerge = statsMerge.withColumn(\n",
    "        'Total', statsMerge['CancelNum'] + statsMerge['NoCancelNum'])\n",
    "    statsMerge = statsMerge.withColumn('CancelRate', bround(\n",
    "        statsMerge['CancelNum'] / statsMerge['Total'], scale=6))\n",
    "\n",
    "    # 打印结果到控制台\n",
    "    statsMerge.show()\n",
    "\n",
    "    # 返回结果 DataFrame\n",
    "    return statsMerge\n",
    "\n",
    "\n",
    "def reverseCapture():\n",
    "    # 获取航班取消率数据\n",
    "    statsMerge = badCancelRate()\n",
    "\n",
    "    # 将 Spark DataFrame 转换为 Pandas DataFrame\n",
    "    pandasDelay = statsMerge.toPandas()\n",
    "\n",
    "    # 绘制取消率柱状图\n",
    "    pandasDelay['CancelRate'].plot(kind='barh', color=['skyblue', 'orange'])\n",
    "    plt.xlabel('Cancel Rate')\n",
    "    plt.ylabel('Flight Type (0: Short, 1: Long)')\n",
    "    plt.title(\"Cancel Rate by Flight Type\")\n",
    "    plt.yticks([0, 1], ['Short', 'Long'])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 执行数据分析并绘制图表\n",
    "reverseCapture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_mean.py\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if 'spark' not in globals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"flightProblem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs://localhost:9000/airport\")\n",
    "\n",
    "data = data.limit(100000)\n",
    "df = data.toPandas()\n",
    "\n",
    "# 数据准备\n",
    "data_X = df[['Month', 'DayofMonth', 'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest', 'Distance']]\n",
    "\n",
    "# 处理分类特征：将字符串编码为数值\n",
    "label_encoders = {}\n",
    "categorical_columns = ['UniqueCarrier', 'Origin', 'Dest']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data_X[col] = le.fit_transform(data_X[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_X)\n",
    "\n",
    "# 使用K-means进行聚类\n",
    "k = 2  # 假设聚类为2类（Cancelled和未Cancelled）\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "# 将聚类结果添加回原数据\n",
    "df['Cluster'] = clusters\n",
    "\n",
    "# 查看聚类结果分布\n",
    "print(\"聚类结果统计：\")\n",
    "print(df.groupby('Cluster')['Cancelled'].value_counts())\n",
    "\n",
    "# 计算轮廓系数（评估聚类质量）\n",
    "silhouette_avg = silhouette_score(data_scaled, clusters)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
    "\n",
    "# 可视化聚类中心\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data_scaled[:, 0], data_scaled[:, 1],\n",
    "            c=clusters, cmap='viridis', s=10)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[\n",
    "            :, 1], c='red', marker='x', s=200)\n",
    "plt.title(\"K-means Clustering Results\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
